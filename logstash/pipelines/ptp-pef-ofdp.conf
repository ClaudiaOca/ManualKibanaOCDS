#
# ENTRADA: Archivos CSV en /etc/logstash/input/ptp-pef-ofdp/
#
input {
  file {
    path => "/etc/logstash/input/ptp-pef-ofdp/*.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    ignore_older => 864000000
    close_older => 2
    max_open_files => 16
  }
}
#
# FILTROS
#
filter {
  # Procesa el archivo CSV, si hay errores agrega el tag "_csvparsefailure"
  csv {
    columns => [ "CICLO","DESC_AI","DESC_CAPITULO","DESC_CONCEPTO","DESC_FF","DESC_FUNCION","DESC_GPO_FUNCIONAL","DESC_MODALIDAD","DESC_PARTIDA_ESPECIFICA","DESC_PARTIDA_GENERICA","DESC_PP","DESC_RAMO","DESC_SUBFUNCION","DESC_TIPOGASTO","DESC_UR","ENTIDAD_FEDERATIVA","GPO_FUNCIONAL","ID_AI","ID_CAPITULO","ID_CLAVE_CARTERA","ID_CONCEPTO","ID_ENTIDAD_FEDERATIVA","ID_FF","ID_FUNCION","ID_MODALIDAD","ID_PARTIDA_ESPECIFICA","ID_PARTIDA_GENERICA","ID_PP","ID_RAMO","ID_SUBFUNCION","ID_TIPOGASTO","ID_UR","MONTO_ADEFAS","MONTO_APROBADO","MONTO_DEVENGADO","MONTO_EJERCICIO","MONTO_EJERCIDO","MONTO_MODIFICADO","MONTO_PAGADO" ]
    skip_header => true
  }
  # Cuenta columnas como validacion simple, 39 columnas de datos + 5 meta columnass, si hay error, agrega el tag "_csvparsefailure"
  ruby {
    code => "raise('Error') unless event.to_hash.length == (39 + 5)"
    tag_on_exception => "_csvparsefailure"
  }
  # Si no hay error hasta ahora, continuamos con la transformacion de datos
  if "_csvparsefailure" not in [tags] {
    mutate {
      # Limpia espacios
      strip => [ "CICLO","DESC_AI","DESC_CAPITULO","DESC_CONCEPTO","DESC_FF","DESC_FUNCION","DESC_GPO_FUNCIONAL","DESC_MODALIDAD","DESC_PARTIDA_ESPECIFICA","DESC_PARTIDA_GENERICA","DESC_PP","DESC_RAMO","DESC_SUBFUNCION","DESC_TIPOGASTO","DESC_UR","ENTIDAD_FEDERATIVA","GPO_FUNCIONAL","ID_AI","ID_CAPITULO","ID_CLAVE_CARTERA","ID_CONCEPTO","ID_ENTIDAD_FEDERATIVA","ID_FF","ID_FUNCION","ID_MODALIDAD","ID_PARTIDA_ESPECIFICA","ID_PARTIDA_GENERICA","ID_PP","ID_RAMO","ID_SUBFUNCION","ID_TIPOGASTO","ID_UR","MONTO_ADEFAS","MONTO_APROBADO","MONTO_DEVENGADO","MONTO_EJERCICIO","MONTO_EJERCIDO","MONTO_MODIFICADO","MONTO_PAGADO" ]
      # Agregamos el tag "_csvparsefailure", sera quitado en el siguiento filtro mutate->convert
      add_tag => [ "_csvparsefailure" ]
    }
    # Los campos con tipo fijo son procesados, y en caso de exito, el tag "_csvparsefailure" es removido
    mutate {
      convert => {
        "CICLO" => "integer"
        "MONTO_ADEFAS" => "float"
        "MONTO_APROBADO" => "float"
        "MONTO_DEVENGADO" => "float"
        "MONTO_EJERCICIO" => "float"
        "MONTO_EJERCIDO" => "float"
        "MONTO_MODIFICADO" => "float"
        "MONTO_PAGADO" => "float"
      }
      remove_tag => [ "_csvparsefailure" ]
    }
    mutate {
      remove_field => [ "path", "host" ]
    }
  }
}
#
# SALIDA
#
output {
  if "_csvparsefailure" in [tags] {
    # En caso de error de analisis o transformacion (tag "_csvparsefailure") agrega al log de failure
    file {
      path => "/etc/logstash/input/ptp-pef-ofdp/failure.log"
      create_if_deleted => true
    }
  } else {
    # Si no hay error envia a un CSV nuevo, con los datos procesados y a ElasticSearch
    file {
      path => "/etc/logstash/input/ptp-pef-ofdp/success.log"
      codec => line { format => "%{message}" }
      create_if_deleted => true
    }
    elasticsearch {
      index => "poder-ptp-pef-ofdp"
      hosts => ["elasticsearch:9200"]
      template => "/etc/logstash/templates/ptp-pef-ofdp.json"
      template_name => "poder-ptp-pef-ofdp"
      template_overwrite => true
      document_id => "%{CICLO}-%{GPO_FUNCIONAL}-%{ID_AI}-%{ID_CAPITULO}-%{ID_CLAVE_CARTERA}-%{ID_CONCEPTO}-%{ID_ENTIDAD_FEDERATIVA}-%{ID_FF}-%{ID_FUNCION}-%{ID_MODALIDAD}-%{ID_PARTIDA_ESPECIFICA}-%{ID_PARTIDA_GENERICA}-%{ID_PP}-%{ID_RAMO}-%{ID_SUBFUNCION}-%{ID_TIPOGASTO}-%{ID_UR}"
    }
  }
}
